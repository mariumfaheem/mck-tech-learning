{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b6a601-2c96-46ed-a857-cb5a9a477a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "users = [(1,\n",
    "  'Corrie',\n",
    "  'Van den Oord',\n",
    "  'cvandenoord0@etsy.com',\n",
    "  True,\n",
    "  1000.55,\n",
    "  datetime.date(2021, 1, 15),\n",
    "  datetime.datetime(2021, 2, 10, 1, 15)),\n",
    " (2,\n",
    "  'Nikolaus',\n",
    "  'Brewitt',\n",
    "  'nbrewitt1@dailymail.co.uk',\n",
    "  True,\n",
    "  900.0,\n",
    "  datetime.date(2021, 2, 14),\n",
    "  datetime.datetime(2021, 2, 18, 3, 33)),\n",
    " (3,\n",
    "  'Orelie',\n",
    "  'Penney',\n",
    "  'openney2@vistaprint.com',\n",
    "  True,\n",
    "  850.55,\n",
    "  datetime.date(2021, 1, 21),\n",
    "  datetime.datetime(2021, 3, 15, 15, 16, 55)),\n",
    " (4,\n",
    "  'Ashby',\n",
    "  'Maddocks',\n",
    "  'amaddocks3@home.pl',\n",
    "  False,\n",
    "  None,\n",
    "  None,\n",
    "  datetime.datetime(2021, 4, 10, 17, 45, 30)),\n",
    " (5,\n",
    "  'Kurt',\n",
    "  'Rome',\n",
    "  'krome4@shutterfly.com',\n",
    "  False,\n",
    "  None,\n",
    "  None,\n",
    "  datetime.datetime(2021, 4, 2, 0, 55, 18))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd63542-4a8e-4d50-9b69-be4b772932bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Help on method createDataFrame in module pyspark.sql.session:\n",
       "\n",
       "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
       "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
       "    \n",
       "    When ``schema`` is a list of column names, the type of each column\n",
       "    will be inferred from ``data``.\n",
       "    \n",
       "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
       "    from ``data``, which should be an RDD of either :class:`Row`,\n",
       "    :class:`namedtuple`, or :class:`dict`.\n",
       "    \n",
       "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
       "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
       "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
       "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be &#34;value&#34;.\n",
       "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
       "    \n",
       "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
       "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
       "    \n",
       "    .. versionadded:: 2.0.0\n",
       "    \n",
       "    .. versionchanged:: 2.1.0\n",
       "       Added verifySchema.\n",
       "    \n",
       "    Parameters\n",
       "    ----------\n",
       "    data : :class:`RDD` or iterable\n",
       "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
       "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
       "        :class:`pandas.DataFrame`.\n",
       "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
       "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
       "        column names, default is None.  The data type string format equals to\n",
       "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
       "        omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n",
       "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
       "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
       "    samplingRatio : float, optional\n",
       "        the sample ratio of rows used for inferring\n",
       "    verifySchema : bool, optional\n",
       "        verify data types of every row against schema. Enabled by default.\n",
       "    \n",
       "    Returns\n",
       "    -------\n",
       "    :class:`DataFrame`\n",
       "    \n",
       "    Notes\n",
       "    -----\n",
       "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n",
       "    &gt;&gt;&gt; spark.createDataFrame(l).collect()\n",
       "    [Row(_1=&#39;Alice&#39;, _2=1)]\n",
       "    &gt;&gt;&gt; spark.createDataFrame(l, [&#39;name&#39;, &#39;age&#39;]).collect()\n",
       "    [Row(name=&#39;Alice&#39;, age=1)]\n",
       "    \n",
       "    &gt;&gt;&gt; d = [{&#39;name&#39;: &#39;Alice&#39;, &#39;age&#39;: 1}]\n",
       "    &gt;&gt;&gt; spark.createDataFrame(d).collect()\n",
       "    [Row(age=1, name=&#39;Alice&#39;)]\n",
       "    \n",
       "    &gt;&gt;&gt; rdd = sc.parallelize(l)\n",
       "    &gt;&gt;&gt; spark.createDataFrame(rdd).collect()\n",
       "    [Row(_1=&#39;Alice&#39;, _2=1)]\n",
       "    &gt;&gt;&gt; df = spark.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n",
       "    &gt;&gt;&gt; df.collect()\n",
       "    [Row(name=&#39;Alice&#39;, age=1)]\n",
       "    \n",
       "    &gt;&gt;&gt; from pyspark.sql import Row\n",
       "    &gt;&gt;&gt; Person = Row(&#39;name&#39;, &#39;age&#39;)\n",
       "    &gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))\n",
       "    &gt;&gt;&gt; df2 = spark.createDataFrame(person)\n",
       "    &gt;&gt;&gt; df2.collect()\n",
       "    [Row(name=&#39;Alice&#39;, age=1)]\n",
       "    \n",
       "    &gt;&gt;&gt; from pyspark.sql.types import *\n",
       "    &gt;&gt;&gt; schema = StructType([\n",
       "    ...    StructField(&#34;name&#34;, StringType(), True),\n",
       "    ...    StructField(&#34;age&#34;, IntegerType(), True)])\n",
       "    &gt;&gt;&gt; df3 = spark.createDataFrame(rdd, schema)\n",
       "    &gt;&gt;&gt; df3.collect()\n",
       "    [Row(name=&#39;Alice&#39;, age=1)]\n",
       "    \n",
       "    &gt;&gt;&gt; spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
       "    [Row(name=&#39;Alice&#39;, age=1)]\n",
       "    &gt;&gt;&gt; spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
       "    [Row(0=1, 1=2)]\n",
       "    \n",
       "    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;a: string, b: int&#34;).collect()\n",
       "    [Row(a=&#39;Alice&#39;, b=1)]\n",
       "    &gt;&gt;&gt; rdd = rdd.map(lambda row: row[1])\n",
       "    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;int&#34;).collect()\n",
       "    [Row(value=1)]\n",
       "    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;boolean&#34;).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "    Traceback (most recent call last):\n",
       "        ...\n",
       "    Py4JJavaError: ...\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n    \n    When ``schema`` is a list of column names, the type of each column\n    will be inferred from ``data``.\n    \n    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n    from ``data``, which should be an RDD of either :class:`Row`,\n    :class:`namedtuple`, or :class:`dict`.\n    \n    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n    the real data, or an exception will be thrown at runtime. If the given schema is not\n    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be &#34;value&#34;.\n    Each record will also be wrapped into a tuple, which can be converted to row later.\n    \n    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 2.1.0\n       Added verifySchema.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n        :class:`pandas.DataFrame`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n    &gt;&gt;&gt; spark.createDataFrame(l).collect()\n    [Row(_1=&#39;Alice&#39;, _2=1)]\n    &gt;&gt;&gt; spark.createDataFrame(l, [&#39;name&#39;, &#39;age&#39;]).collect()\n    [Row(name=&#39;Alice&#39;, age=1)]\n    \n    &gt;&gt;&gt; d = [{&#39;name&#39;: &#39;Alice&#39;, &#39;age&#39;: 1}]\n    &gt;&gt;&gt; spark.createDataFrame(d).collect()\n    [Row(age=1, name=&#39;Alice&#39;)]\n    \n    &gt;&gt;&gt; rdd = sc.parallelize(l)\n    &gt;&gt;&gt; spark.createDataFrame(rdd).collect()\n    [Row(_1=&#39;Alice&#39;, _2=1)]\n    &gt;&gt;&gt; df = spark.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n    &gt;&gt;&gt; df.collect()\n    [Row(name=&#39;Alice&#39;, age=1)]\n    \n    &gt;&gt;&gt; from pyspark.sql import Row\n    &gt;&gt;&gt; Person = Row(&#39;name&#39;, &#39;age&#39;)\n    &gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))\n    &gt;&gt;&gt; df2 = spark.createDataFrame(person)\n    &gt;&gt;&gt; df2.collect()\n    [Row(name=&#39;Alice&#39;, age=1)]\n    \n    &gt;&gt;&gt; from pyspark.sql.types import *\n    &gt;&gt;&gt; schema = StructType([\n    ...    StructField(&#34;name&#34;, StringType(), True),\n    ...    StructField(&#34;age&#34;, IntegerType(), True)])\n    &gt;&gt;&gt; df3 = spark.createDataFrame(rdd, schema)\n    &gt;&gt;&gt; df3.collect()\n    [Row(name=&#39;Alice&#39;, age=1)]\n    \n    &gt;&gt;&gt; spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name=&#39;Alice&#39;, age=1)]\n    &gt;&gt;&gt; spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;a: string, b: int&#34;).collect()\n    [Row(a=&#39;Alice&#39;, b=1)]\n    &gt;&gt;&gt; rdd = rdd.map(lambda row: row[1])\n    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;int&#34;).collect()\n    [Row(value=1)]\n    &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;boolean&#34;).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78285d1b-1545-46c8-8d93-6f172f6a6e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_schema = [\n",
    "    'id INT',\n",
    "    'first_name STRING',\n",
    "    'last_name STRING',\n",
    "    'email STRING',\n",
    "    'is_customer BOOLEAN',\n",
    "    'amount_paid FLOAT',\n",
    "    'customer_from DATE',\n",
    "    'last_updated_ts TIMESTAMP'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9551596f-89ab-4c2a-b5a7-dc5653074a49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[4]: DataFrame[id INT: bigint, first_name STRING: string, last_name STRING: string, email STRING: string, is_customer BOOLEAN: boolean, amount_paid FLOAT: double, customer_from DATE: date, last_updated_ts TIMESTAMP: timestamp]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[4]: DataFrame[id INT: bigint, first_name STRING: string, last_name STRING: string, email STRING: string, is_customer BOOLEAN: boolean, amount_paid FLOAT: double, customer_from DATE: date, last_updated_ts TIMESTAMP: timestamp]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=users_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9c36b6d-82fb-4420-bc4f-f543623ca8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
       "id INT|first_name STRING|last_name STRING|        email STRING|is_customer BOOLEAN|amount_paid FLOAT|customer_from DATE|last_updated_ts TIMESTAMP|\n",
       "+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
       "     1|           Corrie|    Van den Oord|cvandenoord0@etsy...|               true|          1000.55|        2021-01-15|      2021-02-10 01:15:00|\n",
       "     2|         Nikolaus|         Brewitt|nbrewitt1@dailyma...|               true|            900.0|        2021-02-14|      2021-02-18 03:33:00|\n",
       "     3|           Orelie|          Penney|openney2@vistapri...|               true|           850.55|        2021-01-21|      2021-03-15 15:16:55|\n",
       "     4|            Ashby|        Maddocks|  amaddocks3@home.pl|              false|             null|              null|      2021-04-10 17:45:30|\n",
       "     5|             Kurt|            Rome|krome4@shutterfly...|              false|             null|              null|      2021-04-02 00:55:18|\n",
       "+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n|id INT|first_name STRING|last_name STRING|        email STRING|is_customer BOOLEAN|amount_paid FLOAT|customer_from DATE|last_updated_ts TIMESTAMP|\n+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n|     1|           Corrie|    Van den Oord|cvandenoord0@etsy...|               true|          1000.55|        2021-01-15|      2021-02-10 01:15:00|\n|     2|         Nikolaus|         Brewitt|nbrewitt1@dailyma...|               true|            900.0|        2021-02-14|      2021-02-18 03:33:00|\n|     3|           Orelie|          Penney|openney2@vistapri...|               true|           850.55|        2021-01-21|      2021-03-15 15:16:55|\n|     4|            Ashby|        Maddocks|  amaddocks3@home.pl|              false|             null|              null|      2021-04-10 17:45:30|\n|     5|             Kurt|            Rome|krome4@shutterfly...|              false|             null|              null|      2021-04-02 00:55:18|\n+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=users_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df70e88c-28ed-4676-9d4c-d0422831acd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10 Specifying Schema for Spark Dataframe using List",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
